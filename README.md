## Status
![status](https://img.shields.io/badge/status-actively--developed-brightgreen)  
This project is **currently** being developed and improved with additional features, optimizations, and testing.  

0) DONE. 
Set up - folders and deps, parquet files, fill out configs/default.yaml

1 Working on.

WRite core modules and very

- DONE. config_loader: loads configs/default.yaml → Config object
- DONE. data_loader. THIS WAS A PAIN!!
- DONE. prompt_parser
- DONE. HF model wrapper
- DONE. evaluator. This is the runner.py
- DONE. aggregator. This is metrics.py

2 START HERE. Smoke tests???....use Val dataset???? Need to thinknabout. ON run_val_sweep.py file


3 Validation

4 Final test (hold-outscore)

5 Plots

6 finialize README

## Inspiration for this project

On my road to become an AI person, this project came about for two reasons. The first reason came from my own endless curiosity. Optimization was a topic that I read about on the web and wanted to know more. In my own understanding, optimization can mean many things such as accuracy, latency, I/O, etc., the list goes on and on. One component is internal, onesuch is internal knobs like temperature. Another compoent is hardware and cloud services like an NVIDIA or AMD GPU and Google GPU, respectively. The latter componenent cost money, so I thought to myself that I would try to optimize internally. The second reason is that some job postings preferred to have candidates to have some experience in optimization, and although it was NOT a hard requirement, I took the task upon myself to learn something new, out of my own free will, and gain a new perspective and appreciation in the growing, meteoric field of AI.

## Introduction
LLMs can answer multiple‑choice questions reasonably well, but decoding parameters/knobs, especially temperature and  top-p, strongly influence both accuracy and cost. In production, teams tune these knobs to deliver reliable outputs without overspending on tokens. 

This project tunes **inference-time decoding** specifically **temperature** and **top-p** to maximize accuracy while controlling randomness and token cost. We sweep these knobs on a fixed zero-shot prompt across MMLU subjects, select the best setting on the validation split, then freeze it and report final test performance on the test dataset with an emphasis on accuracy and efficiency (accuracy per token).

## Business Case / So What?

The business case is to improve answer accuracy while reducing token spend. It is a an internal config-only change mirroring real world issues in industry. The project answers the quesiton, "Is there a way to optimize how we run the model (decoding settings) while reducing inference costs?" 

LLM inference cost & performance optimization is a hot topic in industry. Companies pay for every token generated by an API. By tuning these knobs to improve accuracy while cutting cost, it can deliver measurable business value.

## Tech Stack


## Dataset
The dataset is called the [Massive Multitask Learning Understanding](https://huggingface.co/datasets/cais/mmlu/tree/main/professional_medicine) (MMLU) dataset that was downloaded from HugginFace. It consist of multi-choice questions of four from multiple fields of educaiton, professional experince, and other branches of knowledge along with correct answers.

I will be using the val and test datasets for this project. The aux_train dataset is not used because it is used for fitting model weights, not for decoding. The the val dataset is a held-out subset and will be used to choose the optimal configuration and used on the test dataset. 

The current landscape of the data are afer cleaning and dropping columns:

Columns:
- `question`: A question based on the four subjects above.
- `choices`: Four possible answer choices given a list
- `answer`: Gold answer

Dimensions:
- `val`: 1531 rows
- `test`: 14042 rows

There is no corpus. This project is closed-book evaluation meaning the model answers from its learned weights (pretraining) with no external documents or retrieval.

## Model

I'm using model `google/gemma-2-2b-it` which is an instruction 2B parameter from HuggingFace. I chose this model because it is lightweight, fast, and local which runs on my Mac MPS. The model itself is also instruction based mearning it is reliable to return a single answer choice with short prompting. I also set one of config settings `max_new_tokens= 1-2` to foce a single letter/digit. 

Meta Product Manager thign on Linkedin By Spencer

## Parameters/Knobs

Two key:

Temperature is a model's **adventurousness**. At low temperature, plays it safe and usually picks the most obvious answer.

High temperature ⇒ takes more chances and may pick less obvious answers.

1) 0.0 → No creativity. Always picks the single most likely answer according to its knowledge.
2) 0.7 → Some creativity. Sometimes picks the 2nd-most likely answer if it’s close in probability.
3) 1.0+ → Lots of creativity. Will occasionally pick even lower-ranked answers.

Need to talk about top-p?



Imagine the model’s answer choices ranked by likelihood: A = 50%, B = 30%, C = 15%, D = 5%.
1) top-p = 1.0 → Consider all possibilities (full list).

2) top-p = 0.9 → Only consider the smallest set of answers whose probabilities add up to 90% (so maybe just A and B in this example).

3) top-p = 0.8 → Even more restrictive — trims the tail.

## Key Metrics

1) Accuracy = percentage of correct vs. gold
2) Stability = std/variance across K seeds (repeat each setting K times, e.g., K=3)
3) Average tokens = prompt+output tokens per question (proxy for cost)
4) Accuracy per Token = efficiency metric


## Experiment

This experiment does a grid sweep, which means systematically testing all combination. So for each (temperature, top-p) pair, I test combinations and pick the best on validation, then freeze and evaluate once on test.

Example grid:
- Temperatures: {0.0, 0.2, 0.5, 0.7}
- Top-p: {0.8, 0.9, 1.0}
--> **12 total settings**

For each setting we:
1) Run the model on the validation set with a fixed zero-shot prompt.
2) Record accuracy, avg tokens, latency.
3) Select the setting with the highest accuracy (tie-break by lower tokens).
4) Evaluate that setting once on the test set.

## Results


## Limitations


## Next Steps