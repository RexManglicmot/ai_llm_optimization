## Status
Project is still undergoing

0) DONE. 
Set up - folders and deps, parquet files, fill out configs/default.yaml

1 Working on.

WRite core modules and very

- DONE. config_loader: loads configs/default.yaml → Config object
- DONE. data_loader. THIS WAS A PAIN!!
- DONE. prompt_parser
- DONE. HF model wrapper

- START HERE. evaluator
 This is the runner.py

- aggregator

2 Smoke tests


3 Validation

4 Final test (hold-outscore)

5 Plots

6

## Inspiration

## Introduciton
LLMs can answer multiple‑choice questions reasonably well, but decoding parameters (how the model chooses its next token) strongly influence both accuracy and cost. In production, teams tune these knobs to deliver reliable quality without overspending on tokens. This project replicates that real‑world workflow on a medical benchmark so the results are measurable and resume‑ready.

The goal of this project is to identify decoding parameter settings that maximize accuracy for medical MCQs while controlling randomness and cost.

Essentially, A small evaluation harness that sweeps decoding knobs (temperature, top-p) on a fixed zero-shot prompt and reports accuracy, stability, and token efficiency on medical MCQs.


Need to talk about temperature?

1) 0.0 → No creativity. Always picks the single most likely answer according to its knowledge.
2) 0.7 → Some creativity. Sometimes picks the 2nd-most likely answer if it’s close in probability.
3) 1.0+ → Lots of creativity. Will occasionally pick even lower-ranked answers.

Need to talk about top-p?
Imagine the model’s answer choices ranked by likelihood: A = 50%, B = 30%, C = 15%, D = 5%.
1) top-p = 1.0 → Consider all possibilities (full list).

2) top-p = 0.9 → Only consider the smallest set of answers whose probabilities add up to 90% (so maybe just A and B in this example).

3) top-p = 0.8 → Even more restrictive — trims the tail.

Need to talk about business case for this project?

Sweeping parameters
“Sweeping” means systematically testing all combinations you care about.

Example:
Temperatures: 0.0, 0.2, 0.5, 0.7
Top-p: 0.8, 0.9, 1.0
That’s 4 × 3 = 12 total setting combinations to try.

For each combination, you:
Run the model on all questions in the dataset.
Record accuracy.
Compare results to find the sweet spot.

In your project, you’re showing you can experiment, measure, and recommend settings.

Need to talk about the grid search?

Why do we care?
The right combination of temperature and top-p can:
Increase accuracy for factual questions (like MMLU Professional Medicine).
Reduce variability between runs (stable results).
Sometimes reduce cost (fewer long-winded answers).

LLM inference cost & performance optimization is a hot skill.
Companies pay for every token generated by an API, and decoding parameters like temperature and top-p directly affect accuracy, creativity, and token count.

If you can tune these knobs to improve accuracy while cutting cost, you deliver measurable business value.

Need to talk about business case for this project?

## Experiment
For each (temperature, top-p) pair:

Run all questions in the Professional Medicine dataset through the model.

Record:

Accuracy (% correct answers)

Stability (if you repeat runs with different seeds)

Tokens used (optional, proxy for cost)

Compare results and find the sweet spot — highest accuracy with good stability.

## Dataset
Please fill this in


## Metrics
1) Accuracy = % correct vs. gold
2) Stability = std/variance across K seeds (repeat each setting K times, e.g., K=3)
3) Average tokens = prompt+output tokens per question (proxy for cost)
4) Accuracy per Token = efficiency metric

Temp | Top-p | Acc(val) | Acc±Std   | AvgTokens | Acc/Token
-----+-------+----------+-----------+-----------+----------
0.0  | 1.0   | 0.672    | 0.672±0.000 | 58.2     | 0.0115
0.2  | 0.9   | 0.708    | 0.708±0.006 | 60.1     | 0.0118
0.5  | 0.9   | 0.701    | 0.701±0.009 | 62.4     | 0.0112
0.7  | 0.8   | 0.665    | 0.665±0.012 | 64.0     | 0.0104



## Deliverables:
1) Heatmap: Accuracy by temperature (x-axis) and top-p (y-axis).
2) Line plot: Accuracy vs. temperature (separate liens per top-p)
3) Bar chsrt: accuracy per token for top 3 settings

## Expected Outcome:

1) Show that too high randomness (temp > 0.7) hurts accuracy.
2) Identify a stable “sweet spot” for medical MCQs (often temp ≈ 0.2–0.5, top-p ≈ 0.9).

Possible Resume Bullet:

Optimized LLM decoding parameters for medical MCQs using the MMLU Professional Medicine dataset. Swept temperature and top-p values to identify settings that improved accuracy by 3.8% while maintaining output stability, delivering a reproducible evaluation framework.


HF website:
https://huggingface.co/datasets/cais/mmlu/tree/main/professional_medicine

